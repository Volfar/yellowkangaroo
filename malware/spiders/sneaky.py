# -*- coding: utf-8 -*-
import os
import scrapy
from urllib.parse import urlparse


class SneakySpider(scrapy.Spider):
    name = 'sneaky'
    start_urls = ['http://example.com/']

    def parse(self, response):
        if response.status == 200:
            try:
                if 'Index of' in response.css('title::text').extract_first():
                    for next_page in response.css('a::attr(href)').extract():
                        if next_page is not None and not str(next_page).startswith("?"):
                            next_page = response.urljoin(next_page)
                            yield scrapy.Request(next_page, callback=self.parse)
            except:
                pass
            dirpath = os.path.join('samples', urlparse(response.url).netloc)
            filepath = os.path.join(dirpath, urlparse(response.url).path.lstrip("/"))
            if not os.path.exists(os.path.dirname(filepath)):
                os.makedirs(os.path.dirname(filepath))
            if not os.path.basename(filepath):
                filepath = os.path.join(os.path.dirname(filepath), "index.html")
            print("Downloading:{0}".format(filepath))
            with open(filepath, "wb") as file:
                file.write(response.body)